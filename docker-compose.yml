services:
  # PostgreSQL Database
  db:
    image: postgres:16-alpine
    container_name: pixel-buddy-db
    restart: unless-stopped
    environment:
      POSTGRES_DB: pixel_buddy
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${DB_PASSWORD:-pixel_secret_2025}
    ports:
      - "${DB_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./db/schema.sql:/docker-entrypoint-initdb.d/schema.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - pixel-buddy-network

  # Ollama AI Service (Local LLM for pet chat)
  ollama:
    image: ollama/ollama:latest
    container_name: pixel-buddy-ollama
    restart: unless-stopped
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - pixel-buddy-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Allow time for model download
    deploy:
      resources:
        limits:
          memory: ${OLLAMA_MEMORY_LIMIT:-4G}
        reservations:
          memory: ${OLLAMA_MEMORY_RESERVE:-2G}
    environment:
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-2}
      - OLLAMA_MAX_QUEUE=${OLLAMA_MAX_QUEUE:-128}
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-5m}
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        #!/bin/bash
        set -e
        echo "ðŸ¤– Starting Ollama AI service..."

        # Start Ollama in background
        /bin/ollama serve &
        OLLAMA_PID=$!

        # Wait for Ollama to be ready
        echo "â³ Waiting for Ollama to start..."
        for i in {1..30}; do
          if curl -s http://localhost:11434/ > /dev/null 2>&1; then
            echo "âœ… Ollama server is ready!"
            break
          fi
          echo "   Attempt $i/30..."
          sleep 2
        done

        # Pull llama3.2:1b if not already present
        echo "ðŸ“¥ Checking for llama3.2:1b model..."
        if ! ollama list | grep -q "llama3.2:1b"; then
          echo "ðŸ“¥ Downloading llama3.2:1b (1.3GB)..."
          ollama pull llama3.2:1b
          echo "âœ… Model ready!"
        else
          echo "âœ… Model already downloaded"
        fi

        echo "ðŸš€ Ollama ready for AI chat!"
        wait $OLLAMA_PID

  # Pixel Buddy App (Development with hot-reload)
  app:
    build:
      context: .
      dockerfile: Dockerfile.dev
    container_name: pixel-buddy-app
    restart: unless-stopped
    depends_on:
      db:
        condition: service_healthy
      # ollama:
      #   condition: service_healthy  # Commented out - using native Ollama instead
    environment:
      DATABASE_URL: postgresql://postgres:${DB_PASSWORD:-pixel_secret_2025}@db:5432/pixel_buddy
      PORT: 3000
      NODE_ENV: development
      OLLAMA_URL: ${OLLAMA_URL:-http://ollama:11434}
      CORS_ORIGIN: "*"
    ports:
      - "${APP_PORT:-3000}:3000"
    volumes:
      # Mount source code for hot-reload
      - ./server.js:/app/server.js
      - ./db:/app/db
      - ./scripts:/app/scripts
      - ./public:/app/public
      - ./auth:/app/auth
      - ./utils:/app/utils
      - ./middleware:/app/middleware
      # Use named volume for node_modules (avoid overwriting)
      - node_modules:/app/node_modules
    networks:
      - pixel-buddy-network
    command: npm run dev

networks:
  pixel-buddy-network:
    driver: bridge

volumes:
  postgres_data:
    driver: local
  node_modules:
    driver: local
  ollama_data:
    driver: local
